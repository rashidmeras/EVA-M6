{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_S4_Assignment1_b.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashidmeras/EVA-M6/blob/master/EVA_S4_Assignment1_b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# EVA (M6) Session4 Assignment: Proposal2\n",
        "\n",
        "Objective:\n",
        "\n",
        "> Using the network defined in *Proposal1* explore different techniques and reduce the total number of parameters such that it is not more than 20K and the validation accuracy is above 99.2%.\n",
        "\n",
        "\n",
        "\n",
        "*So lets Start!!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIK3KZEHAu-",
        "colab_type": "text"
      },
      "source": [
        "Install the keras API library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "263079a3-9bfd-4723-8b36-2aac82eebb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glODJyl3JLIE",
        "colab_type": "text"
      },
      "source": [
        "From Keras API library following APIs are needed to create a DNN:\n",
        "\n",
        "* The sequential API allows to create models layer-by-layer\n",
        "* The Flatten API flattens the input. Does not affect the batch size.\n",
        "* The Convolution2D API creates a convolution kernel that is convolved with the layer input.\n",
        "* The np_utils API is used to convert a class vector (integers) to binary class matrix.\n",
        "* Finally import the MNSIT dataset from Keras\n",
        "\n",
        "MNIST has a training set of 60,000 examples, and a test set of 10,000 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load the the data, shuffled and split between train and test sets.\n",
        "\n",
        ">The MNIST dataset consists of pair, “handwritten digit image” and “label”. Digit ranges from 0 to 9, meaning 10 patterns in total.\n",
        "\n",
        "* handwritten digit image (X_train): This is gray scale image with size 28 x 28 pixel.\n",
        "* label (y_train): This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "5246ddcf-35b7-40ab-b4c2-85bfa2dfd7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYa6Dh5WJ1-o",
        "colab_type": "text"
      },
      "source": [
        "Matplotlib is a Python 2D plotting library & PyPlot is a shell-like interface to Matplotlib\n",
        "\n",
        "Display the data in X_train[0] array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "4f99ca24-2586-48a2-a7f4-0d77865b224b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff2aa665080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuJ7CUzrKA8U",
        "colab_type": "text"
      },
      "source": [
        "Flatten 28x28 images to a 28*28=784 vector for each image.\n",
        "\n",
        "> The images in the dataset are of 28*28 dimensions which is difficult to accommodate in a simple multilayer neural network. Therefore we need to convert the images into a single dimension where each image contains 784-pixel data using the reshape() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciBpETfIKHld",
        "colab_type": "text"
      },
      "source": [
        "The pixel values in the images are in the range of 0 - 255 and in this step we reduce this range even further and normalize it between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWcDTj4QKN3r",
        "colab_type": "text"
      },
      "source": [
        "label : This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "c3c6a746-8cb1-474f-df32-7cbb8aadeb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLywadlZKTIj",
        "colab_type": "text"
      },
      "source": [
        "Convert class vectors to binary class matrices:\n",
        "\n",
        "> As we can see from above, the output of y_train is an integer from 0 to 9. We need to perform one-hot encoding of the class labels for getting a vector of class integers into a binary matrix. We need to do this to do a “binarization” of the category and so that we can include it as a feature to train the neural network.\n",
        "\n",
        "We can use the built in np_utils.to_categorical() helper function in keras to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTmTwTNKiWh",
        "colab_type": "text"
      },
      "source": [
        "Print the Y_train array after binarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "86996bcf-f15b-45d5-c3ca-3b7ab6e33ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LewGS1xoKnyr",
        "colab_type": "text"
      },
      "source": [
        "##Network Type3\n",
        "\n",
        "![Proposed Network Archtecture](https://rashidmeras.github.io/images/eva/S4_Proposal2_Fig1.png)\n",
        "\n",
        "Compared to the *Proposal1* network architecteure, following are the changes proposed here in Network Type3:\n",
        "\n",
        "> 1. The network architecture is basically the same except that the number of layers are reduced. In this proposed network architecture we stop at Receptive Field of 7x7.\n",
        "> 2. The channel size are reduced and now the largest channel size is 32 and the smallest channel size is 12\n",
        "\n",
        "\n",
        "**Why stop at 7x7?**\n",
        "\n",
        "1. Only the central pixels are going to convolved hence there in no much information that network learns by going beyond 7x7\n",
        "2. Going beyond 7x7 data being transferred is reduced to very small section and won't be able to infer useful information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCjHYbiFS_-H",
        "colab_type": "text"
      },
      "source": [
        "Let's fix a random seed=**990** for reproducibility!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4_kOtLTBAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 990\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3Ar0vWTHz5",
        "colab_type": "text"
      },
      "source": [
        "**Network Type 3:**\n",
        "\n",
        "Using the architecture that we have defined above, the new network is implement as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "3964d814-93a3-4e12-d925-21773c1059c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "#Layer1: i/p:|28x28x1|Conv(3x3x1)x32| o/p:|26x26x32|\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', use_bias=False, input_shape=(28,28,1)))\n",
        "\n",
        "#Layer2: i/p:|26x26x32|Conv(3x3x32)x16| o/p:|24x24x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "\n",
        "#Layer3: i/p:|24x24x16|Conv(3x3x16)x16| o/p:|22x22x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "\n",
        "#Layer4: i/p:|22x22x16|Conv(1x1x16)x12| o/p:|22x22x12|\n",
        "model.add(Convolution2D(12, 1, 1, activation='relu', use_bias=False))\n",
        "\n",
        "#Layer5: Max-Pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#Layer6: i/p:|11x11x16|Conv(3x3x16)x16| o/p:|9x9x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "\n",
        "#Layer7: i/p:|9x9x16|Conv(3x3x16)x16| o/p:|7x7x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "\n",
        "#Layer8: i/p:|7x7x16|Conv(7x7x16)x10| o/p:|1x1x10|\n",
        "model.add(Convolution2D(10, 7, 7, use_bias=False))\n",
        "\n",
        "#Layer9: Flatten & activation\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        288       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        2304      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 22, 22, 12)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          1728      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 10)          7840      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,264\n",
            "Trainable params: 19,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", use_bias=False, input_shape=(28, 28, 1...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7), use_bias=False)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_0RKp7mKtc4",
        "colab_type": "text"
      },
      "source": [
        "Compile the model based on following:\n",
        "\n",
        "* Optimization method: Here we use 'adam'\n",
        "* Kind of loss this method will optimize: Here we use 'categorical_crossentropy'\n",
        "\n",
        "Start training the model:\n",
        "\n",
        "* Batch size: set to 128\n",
        "* Epoch: set to 30\n",
        "* Print validation accuracy at each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        },
        "outputId": "9a4009a6-9c36-4bfa-8d11-e3eb429ac0fe"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=30, validation_data=(X_test, Y_test), verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.2635 - acc: 0.9210 - val_loss: 0.0615 - val_acc: 0.9819\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0698 - acc: 0.9789 - val_loss: 0.0459 - val_acc: 0.9861\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0503 - acc: 0.9850 - val_loss: 0.0418 - val_acc: 0.9866\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.0431 - acc: 0.9867 - val_loss: 0.0371 - val_acc: 0.9887\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.0375 - acc: 0.9881 - val_loss: 0.0393 - val_acc: 0.9885\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.0331 - acc: 0.9900 - val_loss: 0.0372 - val_acc: 0.9881\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0282 - val_acc: 0.9912\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0296 - val_acc: 0.9901\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0221 - acc: 0.9931 - val_loss: 0.0298 - val_acc: 0.9908\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0306 - val_acc: 0.9908\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0362 - val_acc: 0.9897\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0165 - acc: 0.9947 - val_loss: 0.0309 - val_acc: 0.9902\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0338 - val_acc: 0.9900\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0146 - acc: 0.9947 - val_loss: 0.0372 - val_acc: 0.9901\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0380 - val_acc: 0.9897\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0354 - val_acc: 0.9896\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0297 - val_acc: 0.9919\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0271 - val_acc: 0.9917\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0305 - val_acc: 0.9911\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0375 - val_acc: 0.9906\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0400 - val_acc: 0.9895\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0375 - val_acc: 0.9907\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0379 - val_acc: 0.9902\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0512 - val_acc: 0.9882\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0599 - val_acc: 0.9876\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0375 - val_acc: 0.9913\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0062 - acc: 0.9977 - val_loss: 0.0402 - val_acc: 0.9909\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0468 - val_acc: 0.9903\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0049 - acc: 0.9982 - val_loss: 0.0488 - val_acc: 0.9894\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0420 - val_acc: 0.9905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2aa6656d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV8ofsVb490O",
        "colab_type": "text"
      },
      "source": [
        "Result:\n",
        "* Total params: **19,264**\n",
        "* Trainable params: 19,264\n",
        "* Non-trainable params: 0\n",
        "\n",
        ">* Score (validation accuracy): **99.19%** at 17th epoch\n",
        "\n",
        "Analysis:\n",
        "> The total number of parameters is **19264** (under 20K) and the validation accuracy is **99.19%** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2n97OrkLffn",
        "colab_type": "text"
      },
      "source": [
        "###Summary:\n",
        "\n",
        "In this section we introduced Network Type3 that has the followiing updates from it's predecessor:\n",
        "1. Reduced channel sizes\n",
        "2. Reduced total layers i.e last layer at 7x7\n",
        "\n",
        "The total epoch updated to 30 and we also added validation_data during model fit which helped us to see the vaildation accuracy at each epoch.\n",
        "\n",
        "Hence in ***Proposal2*** we have updated the previous network discussed in Proposal1 and named it as **Network Type3** this can be further improvised with new techniques to increase the validation accuracy.\n",
        "\n",
        "###Thank you!"
      ]
    }
  ]
}