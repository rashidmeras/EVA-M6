{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_S4_Assignment1_d.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashidmeras/EVA-M6/blob/master/EVA_S4_Assignment1_d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# EVA (M6) Session4 Assignment: The Final Network\n",
        "\n",
        "Objective:\n",
        "\n",
        "> Using different networks discussed in Proposal3 explore different techniques and define a Final Network(s) in which the total number of parameters is less than 15K and the validation accuracy is equal to or above 99.4%.\n",
        "\n",
        "\n",
        "*So let's Start!!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIK3KZEHAu-",
        "colab_type": "text"
      },
      "source": [
        "Install the keras API library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "a0d37423-32c0-4443-92f3-c08e423e229a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glODJyl3JLIE",
        "colab_type": "text"
      },
      "source": [
        "From Keras API library following APIs are needed to create a DNN:\n",
        "\n",
        "* The sequential API allows to create models layer-by-layer\n",
        "* The Flatten API flattens the input. Does not affect the batch size.\n",
        "* The Convolution2D API creates a convolution kernel that is convolved with the layer input.\n",
        "* The np_utils API is used to convert a class vector (integers) to binary class matrix.\n",
        "* Finally import the MNSIT dataset from Keras\n",
        "\n",
        "MNIST has a training set of 60,000 examples, and a test set of 10,000 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load the the data, shuffled and split between train and test sets.\n",
        "\n",
        ">The MNIST dataset consists of pair, “handwritten digit image” and “label”. Digit ranges from 0 to 9, meaning 10 patterns in total.\n",
        "\n",
        "* handwritten digit image (X_train): This is gray scale image with size 28 x 28 pixel.\n",
        "* label (y_train): This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYa6Dh5WJ1-o",
        "colab_type": "text"
      },
      "source": [
        "Matplotlib is a Python 2D plotting library & PyPlot is a shell-like interface to Matplotlib\n",
        "\n",
        "Display the data in X_train[0] array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "073f82a4-c171-4764-94b9-09083fbabf15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4b079bb8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuJ7CUzrKA8U",
        "colab_type": "text"
      },
      "source": [
        "Flatten 28x28 images to a 28*28=784 vector for each image.\n",
        "\n",
        "> The images in the dataset are of 28*28 dimensions which is difficult to accommodate in a simple multilayer neural network. Therefore we need to convert the images into a single dimension where each image contains 784-pixel data using the reshape() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciBpETfIKHld",
        "colab_type": "text"
      },
      "source": [
        "The pixel values in the images are in the range of 0 - 255 and in this step we reduce this range even further and normalize it between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWcDTj4QKN3r",
        "colab_type": "text"
      },
      "source": [
        "label : This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "4260f5bd-6c26-4405-fde3-a396331b9977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLywadlZKTIj",
        "colab_type": "text"
      },
      "source": [
        "Convert class vectors to binary class matrices:\n",
        "\n",
        "> As we can see from above, the output of y_train is an integer from 0 to 9. We need to perform one-hot encoding of the class labels for getting a vector of class integers into a binary matrix. We need to do this to do a “binarization” of the category and so that we can include it as a feature to train the neural network.\n",
        "\n",
        "We can use the built in np_utils.to_categorical() helper function in keras to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTmTwTNKiWh",
        "colab_type": "text"
      },
      "source": [
        "Print the Y_train array after binarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "a5e8dca0-6e15-4199-fa25-83eeb2e965e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LewGS1xoKnyr",
        "colab_type": "text"
      },
      "source": [
        "##Network Type4: Recap \n",
        "\n",
        "![NetworkType4](https://rashidmeras.github.io/images/eva/S4_Proposal3_Fig2.png)\n",
        "\n",
        "The above figure shows Network Type4 that was defined in the previous *Proposal3* where the channels in the layers above Transitional block were reduced to 1/4 th of the original size and the channels of layers below the Transitional block were kept the same. Mentioned below is the performance metrics of this network.\n",
        "\n",
        "* Total params: **11,336**\n",
        "* Trainable params: 11,214\n",
        "* Non-trainable params: 122\n",
        "\n",
        ">* Score (validation accuracy): **99.28%**\n",
        "\n",
        "\n",
        "#Various networks so far..\n",
        "\n",
        "![Channel Size Constarint Types](https://rashidmeras.github.io/images/eva/S4_Proposal3_Table1.png)\n",
        "\n",
        "Using the technique of constraining the size of the channels in Block1 and Block2 we can see in the above table different variants of this network and its the performance.\n",
        "\n",
        "In this Final Chapter we make use of **Network Type4** (refer Row No.4) and **Network Type5** (refer Row No.5) and we will push the limits of these networks and observe the performance. \n",
        "\n",
        "Inorder to do this we will make use these techniques mentioned below:\n",
        "1. New optimizer known as SGD [2] [3] along with loss method known as binary_crossentropy.\n",
        "2. Learning Rate Scheduler [3]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4blAPY98oxde",
        "colab_type": "text"
      },
      "source": [
        "##Network Type 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "4b704367-3a91-44ec-aa34-1d8be9d12f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "#Layer1: Cov->BN->DO i/p:|28x28x1|Conv(3x3x1)x8| o/p:|26x26x8|\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', use_bias=False, input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer2: Cov->BN->DO i/p:|26x26x8|Conv(3x3x8)x4| o/p:|24x24x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer3: Cov->BN->DO i/p:|24x24x4|Conv(3x3x4)x4| o/p:|22x22x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer4: Cov->BN->DO i/p:|22x22x4|Conv(1x1x4)x3| o/p:|22x22x3|\n",
        "model.add(Convolution2D(3, 1, 1, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer5: Max-Pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #11\n",
        "\n",
        "#Layer6: Cov->BN->DO i/p:|11x11x3|Conv(3x3x3)x16| o/p:|9x9x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer7: Cov->BN->DO i/p:|9x9x16|Conv(3x3x16)x16| o/p:|7x7x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer8: Cov->BN->DO i/p:|7x7x16|Conv(7x7x16)x10| o/p:|1x1x10|\n",
        "model.add(Convolution2D(10, 7, 7, use_bias=False)) #1\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer9: Flatten & activation\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", use_bias=False, input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 4)         288       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 24, 24, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 24, 24, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 22, 22, 4)         144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 22, 22, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 22, 22, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 22, 22, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 9, 9, 16)          432       \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 1, 1, 10)          7840      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,336\n",
            "Trainable params: 11,214\n",
            "Non-trainable params: 122\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7), use_bias=False)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_0RKp7mKtc4",
        "colab_type": "text"
      },
      "source": [
        "Compile the model based on following:\n",
        "\n",
        "* Optimization method: Here we use 'SGD'\n",
        "* Kind of loss this method will optimize: Here we use 'binary_crossentropy'\n",
        "* Define a step_decay function that will take the epoch number as input and returns the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 990\n",
        "np.random.seed(seed)\n",
        "\n",
        "# learning rate schedule\n",
        "def step_decay(epoch):\n",
        "\tinitial_lrate = 0.1\n",
        "\tdrop = 0.5\n",
        "\tepochs_drop = 10.0\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate\n",
        "\n",
        "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
        "  \n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=sgd, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr9-gunHK1Tp",
        "colab_type": "text"
      },
      "source": [
        "Start training the model:\n",
        "\n",
        "* Batch size: set to 128\n",
        "* Epoch: set to 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "00bed747-a2c1-4be1-df92-f56f1d18e52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "source": [
        "# learning schedule callback\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [lrate]\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=callbacks_list)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0182 - acc: 0.9916 - val_loss: 0.0040 - val_acc: 0.9986\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0186 - acc: 0.9913 - val_loss: 0.0048 - val_acc: 0.9984\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0183 - acc: 0.9915 - val_loss: 0.0038 - val_acc: 0.9987\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0180 - acc: 0.9917 - val_loss: 0.0047 - val_acc: 0.9984\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0183 - acc: 0.9916 - val_loss: 0.0044 - val_acc: 0.9984\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0181 - acc: 0.9915 - val_loss: 0.0045 - val_acc: 0.9983\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0176 - acc: 0.9917 - val_loss: 0.0044 - val_acc: 0.9984\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0179 - acc: 0.9916 - val_loss: 0.0043 - val_acc: 0.9985\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0176 - acc: 0.9918 - val_loss: 0.0046 - val_acc: 0.9984\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0172 - acc: 0.9918 - val_loss: 0.0041 - val_acc: 0.9987\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0167 - acc: 0.9920 - val_loss: 0.0040 - val_acc: 0.9986\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0173 - acc: 0.9917 - val_loss: 0.0039 - val_acc: 0.9986\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0169 - acc: 0.9919 - val_loss: 0.0040 - val_acc: 0.9988\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0166 - acc: 0.9922 - val_loss: 0.0037 - val_acc: 0.9988\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0166 - acc: 0.9920 - val_loss: 0.0041 - val_acc: 0.9987\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0166 - acc: 0.9920 - val_loss: 0.0037 - val_acc: 0.9988\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0166 - acc: 0.9921 - val_loss: 0.0037 - val_acc: 0.9989\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0169 - acc: 0.9920 - val_loss: 0.0037 - val_acc: 0.9989\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0171 - acc: 0.9917 - val_loss: 0.0037 - val_acc: 0.9989\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0164 - acc: 0.9922 - val_loss: 0.0036 - val_acc: 0.9989\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0162 - acc: 0.9921 - val_loss: 0.0037 - val_acc: 0.9988\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0167 - acc: 0.9920 - val_loss: 0.0036 - val_acc: 0.9989\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0165 - acc: 0.9920 - val_loss: 0.0038 - val_acc: 0.9989\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0159 - acc: 0.9924 - val_loss: 0.0037 - val_acc: 0.9989\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0164 - acc: 0.9921 - val_loss: 0.0035 - val_acc: 0.9989\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0161 - acc: 0.9922 - val_loss: 0.0035 - val_acc: 0.9990\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0166 - acc: 0.9919 - val_loss: 0.0036 - val_acc: 0.9989\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0164 - acc: 0.9921 - val_loss: 0.0036 - val_acc: 0.9989\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0163 - acc: 0.9921 - val_loss: 0.0036 - val_acc: 0.9990\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0163 - acc: 0.9921 - val_loss: 0.0038 - val_acc: 0.9989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ab80bb390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFxvnDxGeq3",
        "colab_type": "text"
      },
      "source": [
        "Result (Network Type4):\n",
        "* Total params: 11,336\n",
        "* Trainable params: 11,214\n",
        "* Non-trainable params: 122\n",
        "\n",
        ">* Score (validation accuracy): **99.9%** at 29th epoch\n",
        "\n",
        "Analysis (Network Type4):\n",
        "> The total number of parameters is **11336** and the validation accuracy is **99.9%** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWh_8fE7K9Ah",
        "colab_type": "text"
      },
      "source": [
        "## Network Type5\n",
        "\n",
        "> Channel size=1/4 in Block1 and Channel size=1/2 in Block2 (Refer Row No.5 in Table above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        },
        "outputId": "ce06de03-eff0-4c1a-e6a0-09e65fdeff95"
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "#Layer1: Cov->BN->DO i/p:|28x28x1|Conv(3x3x1)x8| o/p:|26x26x8|  \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', use_bias=False, input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer2: Cov->BN->DO i/p:|26x26x8|Conv(3x3x8)x4| o/p:|24x24x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer3: Cov->BN->DO i/p:|24x24x4|Conv(3x3x4)x4| o/p:|22x22x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer4: Cov->BN->DO i/p:|22x22x4|Conv(1x1x4)x3| o/p:|22x22x3|\n",
        "model.add(Convolution2D(3, 1, 1, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer5: Max-Pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#Layer6: Cov->BN->DO i/p:|11x11x3|Conv(3x3x3)x8| o/p:|9x9x8|\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer7: Cov->BN->DO i/p:|9x9x8|Conv(3x3x8)x8| o/p:|7x7x8|\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer8: Cov->BN->DO i/p:|7x7x8|Conv(7x7x8)x10| o/p:|1x1x10|\n",
        "model.add(Convolution2D(10, 7, 7, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer9: Flatten & activation\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", use_bias=False, input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7), use_bias=False)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 24, 24, 4)         288       \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 24, 24, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 24, 24, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 22, 22, 4)         144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 22, 22, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 22, 22, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 22, 22, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 11, 11, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 9, 9, 8)           216       \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 9, 9, 8)           32        \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 9, 9, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 7, 7, 8)           576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 7, 7, 8)           32        \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 7, 7, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 1, 1, 10)          3920      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 5,408\n",
            "Trainable params: 5,318\n",
            "Non-trainable params: 90\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHN_1OJLBcK",
        "colab_type": "text"
      },
      "source": [
        "Print the final score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GRSu881eb84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=sgd, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gVuIxCzenKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2091
        },
        "outputId": "524fc7f1-dc75-4f0f-d958-796f67f612b5"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(step_decay, verbose=1)])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0251 - acc: 0.9895 - val_loss: 0.0082 - val_acc: 0.9972\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0254 - acc: 0.9894 - val_loss: 0.0084 - val_acc: 0.9971\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0251 - acc: 0.9895 - val_loss: 0.0074 - val_acc: 0.9974\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0247 - acc: 0.9896 - val_loss: 0.0085 - val_acc: 0.9971\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0241 - acc: 0.9899 - val_loss: 0.0079 - val_acc: 0.9974\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0241 - acc: 0.9898 - val_loss: 0.0065 - val_acc: 0.9976\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0244 - acc: 0.9897 - val_loss: 0.0069 - val_acc: 0.9977\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0239 - acc: 0.9899 - val_loss: 0.0069 - val_acc: 0.9977\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0238 - acc: 0.9898 - val_loss: 0.0086 - val_acc: 0.9971\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0234 - acc: 0.9900 - val_loss: 0.0064 - val_acc: 0.9978\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0226 - acc: 0.9903 - val_loss: 0.0065 - val_acc: 0.9978\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0225 - acc: 0.9903 - val_loss: 0.0066 - val_acc: 0.9976\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0230 - acc: 0.9901 - val_loss: 0.0066 - val_acc: 0.9978\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0226 - acc: 0.9902 - val_loss: 0.0078 - val_acc: 0.9974\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0227 - acc: 0.9902 - val_loss: 0.0067 - val_acc: 0.9977\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0223 - acc: 0.9902 - val_loss: 0.0072 - val_acc: 0.9976\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0225 - acc: 0.9904 - val_loss: 0.0070 - val_acc: 0.9976\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0223 - acc: 0.9902 - val_loss: 0.0071 - val_acc: 0.9976\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.05.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0229 - acc: 0.9902 - val_loss: 0.0067 - val_acc: 0.9978\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0223 - acc: 0.9903 - val_loss: 0.0065 - val_acc: 0.9978\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0220 - acc: 0.9905 - val_loss: 0.0061 - val_acc: 0.9980\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0219 - acc: 0.9905 - val_loss: 0.0063 - val_acc: 0.9980\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0222 - acc: 0.9904 - val_loss: 0.0062 - val_acc: 0.9979\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0217 - acc: 0.9907 - val_loss: 0.0066 - val_acc: 0.9978\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0223 - acc: 0.9902 - val_loss: 0.0063 - val_acc: 0.9978\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0218 - acc: 0.9904 - val_loss: 0.0065 - val_acc: 0.9980\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0220 - acc: 0.9902 - val_loss: 0.0060 - val_acc: 0.9980\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0219 - acc: 0.9904 - val_loss: 0.0060 - val_acc: 0.9980\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.025.\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.0219 - acc: 0.9905 - val_loss: 0.0057 - val_acc: 0.9980\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0125.\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0215 - acc: 0.9905 - val_loss: 0.0062 - val_acc: 0.9978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ab6fd1dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHTCZ3owJhwV",
        "colab_type": "text"
      },
      "source": [
        "Result (Network Type5):\n",
        "* Total params: 5,408\n",
        "* Trainable params: 5,318\n",
        "* Non-trainable params: 90\n",
        "\n",
        ">* Score (validation accuracy): **99.8%** at 29th epoch\n",
        "\n",
        "Analysis (Network Type5):\n",
        "> The total number of parameters is **5408** and the validation accuracy is **99.8%** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2n97OrkLffn",
        "colab_type": "text"
      },
      "source": [
        "###Summary:\n",
        "\n",
        "After defining a basic architecture, splitting the layers into blocks, changing the size of the channels at each block level and using the approach of doing Convolution -> Batch-normalization -> Dropout  at each layer and using learning rate scheduler; in this final section we propose two types of networks i.e. Network Type4 and Network Type5 that exhibits best performance.\n",
        "\n",
        "1. Network Type4: With Total params=11,336 and Validation Accuracy=99.9%\n",
        "2. Network Type5: With Total params=5,408 and Validation Accuracy=99.8%\n",
        "\n",
        "Both these networks performed exceptionally well and meets the target requirement.\n",
        "\n",
        "###Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANEH60G8NAZs",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "\n",
        "1. [Using Learning Rate Schedules](https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/)\n",
        "2. [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)\n",
        "3. [Learning Rate Scheduler and adaptive learning rates](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)"
      ]
    }
  ]
}